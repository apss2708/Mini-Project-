{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1847267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d1d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "033832aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba5dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dcc1dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, tokenizer, max_length):\n",
    "    tokens = tokenizer(example[\"text\"])[:max_length]\n",
    "    return {\"tokens\": tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c613d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:01<00:00, 24283.12 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:00<00:00, 28070.93 examples/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 256\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length})\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3cfbfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.25\n",
    "train_valid_data = train_data.train_test_split(test_size=test_size)\n",
    "train_data = train_valid_data[\"train\"]\n",
    "valid_data = train_valid_data[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd69db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for example in train_data[\"tokens\"]:\n",
    "    counter.update(example)\n",
    "\n",
    "min_freq = 5\n",
    "vocab = {word: i + 2 for i, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "special_tokens = [\"<unk>\", \"<pad>\"]\n",
    "vocab.update({token: i for i, token in enumerate(special_tokens)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c2622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_index = vocab[\"<unk>\"]\n",
    "pad_index = vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaa9202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 18750/18750 [00:03<00:00, 5936.46 examples/s]\n",
      "Map: 100%|██████████| 6250/6250 [00:01<00:00, 6057.08 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:03<00:00, 6500.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def numericalize_example(example, vocab):\n",
    "    ids = [vocab.get(token, unk_index) for token in example[\"tokens\"]]\n",
    "    return {\"ids\": ids}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs={\"vocab\": vocab})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19ac9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format(type=\"torch\", columns=[\"ids\", \"label\"])\n",
    "valid_data.set_format(type=\"torch\", columns=[\"ids\", \"label\"])\n",
    "test_data.set_format(type=\"torch\", columns=[\"ids\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "585ece80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_ids = [torch.tensor(i[\"ids\"]) for i in batch]\n",
    "        batch_ids = nn.utils.rnn.pad_sequence(\n",
    "            batch_ids, padding_value=pad_index, batch_first=True\n",
    "        )\n",
    "        batch_label = torch.tensor([i[\"label\"] for i in batch])\n",
    "        batch = {\"ids\": batch_ids, \"label\": batch_label}\n",
    "        return batch\n",
    "    return collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0bd9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "batch_size = 512\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a5bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim,\n",
    "        n_filters,\n",
    "        filter_sizes,\n",
    "        output_dim,\n",
    "        dropout_rate,\n",
    "        pad_index,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(embedding_dim, n_filters, filter_size)\n",
    "                for filter_size in filter_sizes\n",
    "            ]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        embedded = self.dropout(self.embedding(ids))\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        conved = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [conv.max(dim=-1).values for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=-1))\n",
    "        prediction = self.fc(cat)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5382876",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.25\n",
    "output_dim = len(train_data.unique(\"label\"))\n",
    "filter_sizes = [3, 5, 7]\n",
    "n_filters = 100\n",
    "embedding_dim = 300\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32b218b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_filters=n_filters,\n",
    "    filter_sizes=filter_sizes,\n",
    "    output_dim=output_dim,\n",
    "    dropout_rate=dropout_rate,\n",
    "    pad_index=pad_index,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0c7670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06e22649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch[\"ids\"])\n",
    "        loss = criterion(predictions, batch[\"label\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            predictions = model(batch[\"ids\"])\n",
    "            loss = criterion(predictions, batch[\"label\"])\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d34454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/q0m5t1y91ds296s1c7f_6w1c0000gn/T/ipykernel_72114/3812657777.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_ids = [torch.tensor(i[\"ids\"]) for i in batch]\n",
      "  0%|          | 0/37 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m valid_losses = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     train_loss = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     valid_loss = evaluate_model(model, valid_data_loader, criterion)\n\u001b[32m      7\u001b[39m     train_losses.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, data_loader, optimizer, criterion)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[32m      5\u001b[39m     optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     loss = criterion(predictions, batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      8\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mCNN.forward\u001b[39m\u001b[34m(self, ids)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     embedded = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     25\u001b[39m     embedded = embedded.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     26\u001b[39m     conved = [torch.relu(conv(embedded)) \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_model(model, train_data_loader, optimizer, criterion)\n",
    "    valid_loss = evaluate_model(model, valid_data_loader, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c846c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (3,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrain Loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, n_epochs+\u001b[32m1\u001b[39m), valid_losses, label=\u001b[33m\"\u001b[39m\u001b[33mValid Loss\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mEpoch\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/matplotlib/pyplot.py:3827\u001b[39m, in \u001b[36mplot\u001b[39m\u001b[34m(scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   3819\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.plot)\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\n\u001b[32m   3821\u001b[39m     *args: \u001b[38;5;28mfloat\u001b[39m | ArrayLike | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3825\u001b[39m     **kwargs,\n\u001b[32m   3826\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[32m-> \u001b[39m\u001b[32m3827\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3828\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3829\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3831\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3832\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3833\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1777\u001b[39m, in \u001b[36mAxes.plot\u001b[39m\u001b[34m(self, scalex, scaley, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1534\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1535\u001b[39m \u001b[33;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[32m   1536\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1774\u001b[39m \u001b[33;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1776\u001b[39m kwargs = cbook.normalize_kwargs(kwargs, mlines.Line2D)\n\u001b[32m-> \u001b[39m\u001b[32m1777\u001b[39m lines = [*\u001b[38;5;28mself\u001b[39m._get_lines(\u001b[38;5;28mself\u001b[39m, *args, data=data, **kwargs)]\n\u001b[32m   1778\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[32m   1779\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_line(line)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:297\u001b[39m, in \u001b[36m_process_plot_var_args.__call__\u001b[39m\u001b[34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     this += args[\u001b[32m0\u001b[39m],\n\u001b[32m    296\u001b[39m     args = args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MINI PROJECT/pytorch-sentiment-analysis-main/venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:494\u001b[39m, in \u001b[36m_process_plot_var_args._plot_args\u001b[39m\u001b[34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[39m\n\u001b[32m    491\u001b[39m     axes.yaxis.update_units(y)\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.shape[\u001b[32m0\u001b[39m] != y.shape[\u001b[32m0\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y must have same first dimension, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y.ndim > \u001b[32m2\u001b[39m:\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mx and y can be no greater than 2D, but have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: x and y must have same first dimension, but have shapes (3,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, n_epochs+1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, n_epochs+1), valid_losses, label=\"Valid Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
