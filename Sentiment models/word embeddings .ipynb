{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe390a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 0.6787 | Acc: 57.90%\n",
      "Valid Loss: 0.6001 | Acc: 69.40%\n",
      "Epoch 2\n",
      "Train Loss: 0.5984 | Acc: 67.84%\n",
      "Valid Loss: 0.4883 | Acc: 78.68%\n",
      "Test Loss: 0.5106 | Acc: 76.81%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1. Download and load GloVe embeddings manually\n",
    "def load_glove(embedding_dim=100):\n",
    "    # Download GloVe embeddings if not exists\n",
    "    if not os.path.exists('glove.6B.100d.txt'):\n",
    "        url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        with open('glove.6B.zip', 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "    \n",
    "    # Load embeddings\n",
    "    embeddings = defaultdict(lambda: np.random.normal(scale=0.6, size=(embedding_dim,)))\n",
    "    embeddings['<pad>'] = np.zeros(embedding_dim)\n",
    "    embeddings['<unk>'] = np.zeros(embedding_dim)\n",
    "    \n",
    "    with open(f'glove.6B.{embedding_dim}d.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# 2. Create vocabulary and embedding matrix\n",
    "class Vocabulary:\n",
    "    def __init__(self, embedding_dim=100):\n",
    "        self.embeddings = load_glove(embedding_dim)\n",
    "        self.stoi = {'<pad>': 0, '<unk>': 1}\n",
    "        self.itos = {0: '<pad>', 1: '<unk>'}\n",
    "        self.vectors = [np.zeros(100), np.zeros(100)]\n",
    "        \n",
    "        # Build vocabulary\n",
    "        for idx, word in enumerate(self.embeddings.keys(), start=2):\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            self.vectors.append(self.embeddings[word])\n",
    "            \n",
    "        self.vectors = torch.tensor(np.array(self.vectors), dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "\n",
    "# 3. Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokens = self.tokenize(text)\n",
    "        indices = [self.vocab.stoi.get(token, 1) for token in tokens]  # 1 = <unk>\n",
    "        \n",
    "        # Pad/truncate\n",
    "        if len(indices) < self.max_len:\n",
    "            indices += [0] * (self.max_len - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_len]\n",
    "            \n",
    "        return torch.tensor(indices), torch.tensor(label)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        return text.split()\n",
    "\n",
    "# 4. Load IMDB dataset manually\n",
    "def load_imdb():\n",
    "    imdb = load_dataset('imdb')\n",
    "    train_valid = imdb['train'].train_test_split(test_size=0.2, seed=1234)\n",
    "    return {\n",
    "        'train_texts': train_valid['train']['text'],\n",
    "        'train_labels': train_valid['train']['label'],\n",
    "        'valid_texts': train_valid['test']['text'],\n",
    "        'valid_labels': train_valid['test']['label'],\n",
    "        'test_texts': imdb['test']['text'],\n",
    "        'test_labels': imdb['test']['label']\n",
    "    }\n",
    "\n",
    "data = load_imdb()\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# 5. Create datasets and dataloaders\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 100\n",
    "\n",
    "train_dataset = TextDataset(data['train_texts'], data['train_labels'], vocab, MAX_LEN)\n",
    "valid_dataset = TextDataset(data['valid_texts'], data['valid_labels'], vocab, MAX_LEN)\n",
    "test_dataset = TextDataset(data['test_texts'], data['test_labels'], vocab, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 6. Model definition\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors, freeze=False, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2], hidden[-1]), dim=1))\n",
    "        return self.fc(hidden)\n",
    "\n",
    "model = TextClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=256,\n",
    "    num_classes=1\n",
    ").to(device)\n",
    "\n",
    "# 7. Training setup\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 8. Training loop\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        correct += (preds == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "# Run training\n",
    "N_EPOCHS = 2\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch+1}')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Acc: {train_acc*100:.2f}%')\n",
    "    print(f'Valid Loss: {valid_loss:.4f} | Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "# Final evaluation\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f} | Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
