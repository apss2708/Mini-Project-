{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a583d49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
      "great 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
      "awesome 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
      "bad -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "terrible -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "awful -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0 -1.0\n",
      "kwyjibo 0.5 -0.5 0.5 -0.5 0.5 -0.5 0.5 -0.5 0.5 -0.5 0.5 -0.5 0.5 -0.5 0.5 -0.5 0.5 -0.5 0.5 -0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('custom_embeddings/embeddings.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set seed for reproducibility and device\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # ADDED DEVICE DEFINITION\n",
    "\n",
    "# 1. Custom Vocabulary and Embedding Loader\n",
    "class Vocabulary:\n",
    "    def __init__(self, embedding_file):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "        self.vectors = []\n",
    "        self.load_embeddings(embedding_file)\n",
    "        \n",
    "    def load_embeddings(self, file_path):\n",
    "        # Add special tokens\n",
    "        self.add_word('<pad>', torch.zeros(20))  # Match embedding_dim\n",
    "        self.add_word('<unk>', torch.zeros(20))\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in tqdm(f, desc=\"Loading embeddings\"):\n",
    "                parts = line.rstrip().split(' ')\n",
    "                word = parts[0]\n",
    "                vector = torch.tensor([float(x) for x in parts[1:]], dtype=torch.float)\n",
    "                self.add_word(word, vector)\n",
    "    \n",
    "    def add_word(self, word, vector):\n",
    "        idx = len(self.itos)\n",
    "        self.stoi[word] = idx\n",
    "        self.itos[idx] = word\n",
    "        self.vectors.append(vector)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vectors)\n",
    "\n",
    "# 2. Custom Dataset Class\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_length=100):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "        label = float(self.data[idx]['label'])\n",
    "        \n",
    "        # Simple tokenizer\n",
    "        tokens = text.lower().translate(\n",
    "            str.maketrans('', '', string.punctuation)\n",
    "        ).split()[:self.max_length]\n",
    "        \n",
    "        # Convert to indices\n",
    "        indices = [self.vocab.stoi.get(token, self.vocab.stoi['<unk>']) \n",
    "                  for token in tokens]\n",
    "        \n",
    "        # Pad/truncate\n",
    "        if len(indices) < self.max_length:\n",
    "            indices += [self.vocab.stoi['<pad>']] * (self.max_length - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_length]\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# 3. Load Data and Vocabulary\n",
    "# Load IMDB dataset\n",
    "imdb = load_dataset('imdb')\n",
    "train_valid = imdb['train'].train_test_split(test_size=0.2, seed=SEED)\n",
    "train_data = train_valid['train']\n",
    "valid_data = train_valid['test']\n",
    "test_data = imdb['test']\n",
    "\n",
    "# Load custom embeddings\n",
    "vocab = Vocabulary('custom_embeddings/embeddings.txt')\n",
    "\n",
    "# 4. Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 100\n",
    "\n",
    "train_dataset = IMDBDataset(train_data, vocab, MAX_LENGTH)\n",
    "valid_dataset = IMDBDataset(valid_data, vocab, MAX_LENGTH)\n",
    "test_dataset = IMDBDataset(test_data, vocab, MAX_LENGTH)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    return torch.stack(texts).to(device), torch.stack(labels).to(device)  # Move to device\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# 5. Model Definition\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n_filters, (fs, embedding_dim)) \n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize with custom embeddings\n",
    "        self.embedding.weight.data.copy_(torch.stack(vocab.vectors))\n",
    "        self.embedding.weight.requires_grad = False  # Start frozen\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).unsqueeze(1)  # Add channel dimension\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat).squeeze()\n",
    "\n",
    "# Model parameters\n",
    "EMBEDDING_DIM = 20\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = vocab.stoi['<pad>']\n",
    "\n",
    "model = CNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    n_filters=N_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(device)\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "# 6. Training Setup\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 7. Training Loop\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for texts, labels in tqdm(loader, desc=\"Training\"):\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        correct += (preds == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in tqdm(loader, desc=\"Evaluating\"):\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "# Training execution\n",
    "N_EPOCHS = 10\n",
    "FREEZE_FOR = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Unfreeze embeddings after FREEZE_FOR epochs\n",
    "    if epoch == FREEZE_FOR:\n",
    "        model.embedding.weight.requires_grad = True\n",
    "        print(\"\\nUnfreezing embeddings!\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    print(f'\\nEpoch {epoch+1:02}')\n",
    "    print(f'Train Loss: {train_loss:.3f} | Acc: {train_acc*100:.2f}%')\n",
    "    print(f'Valid Loss: {valid_loss:.3f} | Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "# Final evaluation\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'\\nFinal Test Performance:')\n",
    "print(f'Loss: {test_loss:.3f} | Acc: {test_acc*100:.2f}%')\n",
    "\n",
    "# Save trained embeddings\n",
    "def save_embeddings(model, vocab, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        for idx in tqdm(range(len(vocab)), desc=\"Saving embeddings\"):\n",
    "            word = vocab.itos[idx]\n",
    "            vector = model.embedding.weight.data[idx].cpu().numpy()\n",
    "            line = f\"{word} \" + \" \".join(map(str, vector)) + \"\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "save_embeddings(model, vocab, 'custom_embeddings/trained_embeddings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638183ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, vocab, sentence, device, max_length=100):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess input text\n",
    "    tokens = sentence.lower().translate(\n",
    "        str.maketrans('', '', string.punctuation)\n",
    "    ).split()[:max_length]\n",
    "    \n",
    "    # Convert to indices\n",
    "    indices = [vocab.stoi.get(token, vocab.stoi['<unk>']) for token in tokens]\n",
    "    \n",
    "    # Pad/truncate\n",
    "    if len(indices) < max_length:\n",
    "        indices += [vocab.stoi['<pad>']] * (max_length - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_length]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    tensor = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(tensor)\n",
    "        probability = torch.sigmoid(output).item()\n",
    "    \n",
    "    return \"Positive\" if probability > 0.5 else \"Negative\", probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115c2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this movie worth watching?\n",
      "Sentiment: Positive | Confidence: 0.5336\n",
      "\n",
      "Statement: Why would anyone like this terrible film?\n",
      "Sentiment: Negative | Confidence: 0.1220\n",
      "\n",
      "Neutral: What time does the movie start?\n",
      "Sentiment: Positive | Confidence: 0.5336\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Direct question\n",
    "question = \"Is this movie worth watching?\"\n",
    "sentiment, confidence = predict_sentiment(model, vocab, question, device)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Sentiment: {sentiment} | Confidence: {confidence:.4f}\")\n",
    "\n",
    "# Example 2: Statement with question mark\n",
    "statement = \"Why would anyone like this terrible film?\"\n",
    "sentiment, confidence = predict_sentiment(model, vocab, statement, device)\n",
    "print(f\"\\nStatement: {statement}\")\n",
    "print(f\"Sentiment: {sentiment} | Confidence: {confidence:.4f}\")\n",
    "\n",
    "# Example 3: Neutral question\n",
    "neutral = \"What time does the movie start?\"\n",
    "sentiment, confidence = predict_sentiment(model, vocab, neutral, device)\n",
    "print(f\"\\nNeutral: {neutral}\")\n",
    "print(f\"Sentiment: {sentiment} | Confidence: {confidence:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
